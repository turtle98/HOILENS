{
  "weight_decay": 0.0001,
  "lr_drop": 10,
  "clip_max_norm": 0.1,
  "backbone": "resnet50",
  "dilation": false,
  "amp": false,
  "position_embedding": "sine",
  "repr_dim": 512,
  "hidden_dim": 256,
  "enc_layers": 6,
  "dec_layers": 6,
  "dim_feedforward": 2048,
  "dropout": 0.1,
  "nheads": 8,
  "num_queries": 100,
  "pre_norm": false,
  "aux_loss": true,
  "set_cost_class": 1,
  "set_cost_bbox": 5,
  "set_cost_giou": 2,
  "bbox_loss_coef": 5,
  "giou_loss_coef": 2,
  "eos_coef": 0.1,
  "cache": false,
  "box_score_thresh": 0.2,
  "fg_iou_thresh": 0.5,
  "min_instances": 3,
  "max_instances": 15,
  "clip_visual_layers_vit": 24,
  "clip_visual_output_dim_vit": 768,
  "clip_visual_input_resolution_vit": 336,
  "clip_visual_width_vit": 1024,
  "clip_visual_patch_size_vit": 14,
  "clip_text_transformer_width_vit": 768,
  "clip_text_transformer_heads_vit": 12,
  "clip_text_transformer_layers_vit": 12,
  "clip_text_context_length_vit": 77,
  "feat_mask_type": 0,
  "repeat_factor_sampling": false,
  "dataset_file": "coco",
  "d_detr": false,
  "lr_backbone": 2e-05,
  "masks": false,
  "with_box_refine": false,
  "two_stage": false,
  "frozen_weights": null,
  "position_embedding_scale": 6.283185307179586,
  "num_feature_levels": 4,
  "dec_n_points": 4,
  "enc_n_points": 4,
  "mask_loss_coef": 1,
  "dice_loss_coef": 1,
  "cls_loss_coef": 2,
  "focal_alpha": 0.25,
  "lr_head": 0.001,
  "lr_vit": 0.001,
  "zs": true,
  "zs_type": "unseen_object",
  "dataset": "hicodet",
  "partitions": [
    "train2015",
    "test2015"
  ],
  "num_classes": 117,
  "data_root": "./hicodet",
  "epochs": 20,
  "batch_size": 4,
  "num_workers": 4,
  "eval": false,
  "clip_dir_vit": "checkpoints/pretrained_clip/ViT-L-14-336px.pt",
  "pretrained": "checkpoints/pretrained_detr/detr-r50-hicodet.pth",
  "resume": "",
  "output_dir": "eval_only_noshortcut",
  "use_hotoken": true,
  "use_prior": true,
  "use_exp": true,
  "alpha": 0.5,
  "gamma": 0.2,
  "hyper_lambda": 2.8,
  "use_insadapter": true,
  "adapter_num_layers": 1,
  "adapt_dim": 128,
  "adapter_alpha": 1.0,
  "adapter_pos": "all",
  "adapter_scalar": "learnable_scalar",
  "use_prompt": true,
  "N_CTX": 36,
  "CSC": true,
  "CTX_INIT": "",
  "CLASS_TOKEN_POSITION": "end",
  "job_id": 1985,
  "vis": false,
  "debug": false,
  "seed": 66,
  "device": "cuda",
  "port": "7894",
  "print_interval": 100,
  "world_size": 4,
  "linear": false,
  "linear_shortcut": false,
  "vision_tower_name": "openai/clip-vit-large-patch14-336",
  "local_rank": 1,
  "clip_model_name": "ViT-L/14@336px",
  "human_idx": 0
}